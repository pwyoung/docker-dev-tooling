#!/usr/bin/env bash

set -e

################################################################################

# Docker image for dev server container
export set IMAGE="dev-tools:latest"

# Name for the container when running
export set CONTAINER_NAME='dev'

# Local SSH port to the dev container's SSH server
export set LOCAL_SSH_PORT='2222'

# ~/.ssh/config entry providing passwordless SSH to the container
export set SSH_ALIAS='dev'

################################################################################

run_container() {
    # Run the docker container.
    #
    # Map ~/home_dev to /home/dev so that files can be edited in the host or container
    #
    # Start the SSH server.

    # Docker run args
    DARGS=""

    # Map the SSH server port
    #
    # Only allow local connections
    #DARGS+=" -p 127.0.0.1:${LOCAL_SSH_PORT}:22/tcp"
    #
    # Allow connections from outside this host
    DARGS+=" -p ${LOCAL_SSH_PORT}:22"

    # Run with an interactive terminal
    #DARGS+=" -t -i"
    # OR,
    # Run as a Daemon/Background process
    DARGS+=" -d"

    # Remove the container when it exits
    DARGS+=" --rm"

    # Share files here
    DARGS+=" -v ${HOME}/home_dev:/home/dev"

    # Set hostname
    DARGS+=" --hostname $CONTAINER_NAME"

    # Set a name for the container
    DARGS+=" --name $CONTAINER_NAME"

    # Deal with a podman issue:
    # ~/home_dev->~dev bind mount being owned by root in the container
    #
    # https://github.com/containers/podman/issues/2898
    #DARGS+=" --uidmap 1000:0:1 --uidmap 0:1:1000"
    # This did not deal with the 1001 gid
    #
    if docker --version | grep -i podman &>/dev/null; then
        # This fails with Docker-CE
        DARGS+=" --userns=keep-id"
    fi

    # Access GPU (if nvidia runtime is set up)
    # https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/latest/user-guide.html
    # For this: https://github.com/NVIDIA/nvidia-container-toolkit
    if docker info | grep Runtime | grep nvidia &>/dev/null; then
        DARGS+=" --gpus all"
        #DARGS+=" --ipc=host --ulimit memlock=-1 --ulimit stack=67108864" # 64 MB
        #DARGS+=" --ipc=host --ulimit memlock=-1 --ulimit stack=2147483648" # 2 GB
        DARGS+=" --ipc=host --ulimit memlock=-1 --ulimit stack=4294967296" # 4 GB
    fi

    # Start SSH server, run a sleep command forever, and return immediately
    CMD="sudo service ssh start && service ssh status && nohup sleep infinity"
    echo "COMMAND:    docker run $DARGS $IMAGE bash -c \"$CMD\"" > /tmp/dev.cmd.out
    docker run $DARGS $IMAGE bash -c "$CMD"
}

# Ensure that the dev server is running
ensure_server_is_running() {
    if ! docker inspect $CONTAINER_NAME 2>/dev/null | jq -r '.[0].State.Status' | grep 'running' >/dev/null; then
        run_container
    fi
}

# Wait for the container to allow SSH connections
wait_for_ssh() {
    for i in {1..9}; do
        echo "Attempt to SSH to $SSH_ALIAS number $i"
        ssh $SSH_ALIAS 'hostname' && break
        sleep 1
    done

    if [[ $i -eq 9 ]]; then
        echo "$SSH_ALIAS container is not responding"
        exit 1
    fi
}

# Run the proxy in the dev container (in the background)
run_mitm_web() {
    if ! ssh $SSH_ALIAS "ps | grep mitmweb" &>/dev/null; then
        echo "Running mitweb"
        ssh $SSH_ALIAS "nohup mitmweb &>/dev/null &"
    fi
}

# Kill the SSH tunnels that we created
kill_our_ssh_tunnels() {
    PIDS=$(ps eaux | grep -v grep | grep PID_ID_TAG='our-ssh-tunnel' | awk '{print $2}')
    for i in $PIDS; do
        echo "Killing pid $i"
        kill -15 $i
    done
}

# Create SSH tunnel (from the host to the container)
create_tunnel() {
    host_port="$1"
    container_port="$2"

    tunnel_label="$host_port -> $container_port"

    if ps x | grep ssh | grep "$host_port:localhost:$container_port" &>/dev/null; then
        echo "Tunnel exists: $tunnel_label"
    else
        echo "Creating tunnel: $tunnel_label"
        export PID_ID_TAG='our-ssh-tunnel' && ssh -f -N -L $host_port:localhost:$container_port $SSH_ALIAS
    fi
}

# Set up SSH tunnels to the container
# We could just expose ports with Docker.
# Keep this code in case we want to manually manage tunnels
create_proxy_tunnels() {
    # MITMPROXY
    create_tunnel "8080" "8080"
    create_tunnel "8081" "8081"
    create_tunnel "4200" "4200"
}

# Open a browser (to the mitm proxy web server)
open_browser() {
    if command -v google-chrome; then
        google-chrome 'http://localhost:8081'
    elif command -v open; then
        open 'http://localhost:8081'
    fi
}

# Show what ports in the dev container are listening
show_ports_open(){
    echo "Ports open in container"
    run_cmd_in_devserver_container "netstat -tulpn | egrep -v '^tcp6'"
}

# Install the mitmproxy CA cert
# https://docs.mitmproxy.org/stable/concepts-certificates/
install_mitm_cert() {
    if docker exec -i -t dev bash -l -c "ls -l ~/.mitmproxy/mitmproxy-ca-cert.pem"; then
        echo "mitmproxy cert is already installed"
        return
    fi

    # Tell CLI tools (that respect this standard) to use these env vars
    CMD="export http_proxy=127.0.0.1:8080 && export https_proxy=127.0.0.1:8080"

    # Test that curl will trust the proxy's cert when we call through the proxy to a test site.
    CMD="$CMD && curl --proxy 127.0.0.1:8080 --cacert ~/.mitmproxy/mitmproxy-ca-cert.pem https://example.com/"

    docker exec -i -t $CONTAINER_NAME bash -l -c "$CMD"
}

# Open a shell in the container with the proxy env vars setup
open_proxying_shell_in_dev_container() {
    # Tell CLI tools (that respect this standard) to use these env vars
    CMD="export http_proxy=127.0.0.1:8080 && export https_proxy=127.0.0.1:8080"

    # Tell the AWS CLI tools to trust the proxy's cert since we will go through it
    CMD="$CMD && export AWS_CA_BUNDLE=~/.mitmproxy/mitmproxy-ca-cert.pem"

    # Make a test AWS CLI call
    #CMD="$CMD && aws sts get-caller-identity"

    # Open a login shell
    CMD="$CMD && bash -l"

    docker exec -i -t $CONTAINER_NAME bash -l -c "$CMD"
}

run_server_with_proxy() {
    ensure_server_is_running

    wait_for_ssh

    run_mitm_web

    install_mitm_cert

    create_proxy_tunnels

    open_browser

    show_ports_open

    open_proxying_shell_in_dev_container
}

run_cmd_in_devserver_container() {
    CMD="$1"

    ensure_server_is_running

    docker exec -i -t $CONTAINER_NAME bash -l -c "$CMD"
}


# Start an ephemeral container and run the given command in it
run_cmd_in_ephemeral_container() {
    CMD="$1"

    # Docker run args
    DARGS=""

    # Run with an interactive terminal
    DARGS+=" -t -i"
    # OR,
    # Run as a daemon/background process (not interactive)
    #DARGS+=" -d"

    # Remove the container when it exits
    DARGS+=" --rm"

    # Share files here
    DARGS+=" -v ${HOME}/home_dev:/home/dev"

    # Set hostname
    DARGS+=" --hostname $CONTAINER_NAME"

    # Specify the command to run
    docker run $DARGS $IMAGE $CMD
}

kill_server() {
    kill_our_ssh_tunnels

    docker kill $CONTAINER_NAME &>/dev/null || true
}

login_to_server() {
    ensure_server_is_running

    wait_for_ssh

    # These all work
    #run_cmd_in_devserver_container "bash -l"
    #ssh -F /dev/null -o "StrictHostKeychecking=no" -o "IdentitiesOnly=yes" -i ~/home_dev/.ssh/id_rsa -p $LOCAL_SSH_PORT -l dev localhost
    ssh $SSH_ALIAS
}

run_jupyter() {
    ensure_server_is_running

    # JUPYTER defaults to 8888
    create_tunnel "8888" "8888"

    # Run a project to run a Jupyter server
    CMD='cd ~/git/nemo-work/tests/pytorch-setup && make'
    ssh $SSH_ALIAS "$CMD"
}

kill_jupyter() {
    ensure_server_is_running

    # Run a project to kill a Jupyter server
    CMD='cd ~/git/nemo-work/tests/pytorch-setup && make clean'
    ssh $SSH_ALIAS "$CMD"
}

show_usage() {
    cat <<EOF
    $0 [-h|--help]
    $0 [-l|--login-to-server]
    $0 [-s|--run-server]
    $0 [-p|--run-server-with-proxy]
    $0 [-j|--run-jupyter]
    $0 [-J|--kill-jupyter]
    $0 [-c <CMD>|--run-command-in-server <CMD>]
    $0 [-e <CMD>|--run-command-in-ephemeral-container <CMD>]
    $0 [-k|--kill-server]
    $0 [-T|--kill-our-ssh-tunnels]
    $0 [-t|--create-tunnels]

    GOAL
      Create a dev container that can support:
      - devcontainer development, e.g. via ssh from Jetbrains or VsCode.
      - local dev/test via various CLI tools. See Dockerfiles for details.
      - REST API debugging see run_server_with_proxy function in this for details.
        This currently supports HTTP and HTTPS (e.g. AWS CLI) calls
    DETAILS
      - User 'dev' is a normal/non-root user that is set up to run code
      - The host dir ~/home_dev is mapped to /home/dev in the container so that:
        - configuration and secrets persist, but are not in version control
        - files can be edited in the host or container
        - files can be run in the host or container
      - Note,
         virtual-environments (e.g. created by venv in the container) can be
         accessed from the host (e.g. by an IDE in the host)
EOF
}

if [[ $# -eq 0 ]]; then
    show_usage
fi

while [[ $# -gt 0 ]]; do
  key="$1"

  case $key in
    -h|--help)
        shift
        show_usage
      ;;
    -l|--login-to-server)
        shift
        login_to_server
        ;;
    -s|--run-server)
        shift
        ensure_server_is_running
        ;;
    -p|--run-server-with-proxy)
        shift
        run_server_with_proxy
        ;;
    -j|--run-jupyter)
        shift
        run_jupyter
        ;;
    -J|--kill-jupyter)
        shift
        kill_jupyter
        ;;
    -c|--run-command-in-server)
        shift
        run_cmd_in_devserver_container "$@"
        ;;
    -e|--run-command-in-ephemeral-container)
        shift
        run_cmd_in_ephemeral_container "$@"
        ;;
    -k|--kill-server)
        shift
        kill_server
        ;;
    -T|--kill-our-ssh-tunnels)
        shift
        kill_our_ssh_tunnels
        ;;
    -t|--create-tunnels)
        shift
        create_tunnels
        ;;
    *)    # unknown option
      POSITIONAL+=("$1") # save it in an array for later
      shift # past argument
      ;;
  esac
done
